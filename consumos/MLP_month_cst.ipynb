{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nRSlxT4q5EHJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-06 23:06:54.670538: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-06 23:06:54.670568: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import regularizers\n",
    "from numpy import array\n",
    "from datetime import datetime\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "cm = 1/2.54 \n",
    "colors = ['#4477AA', '#EE6677', '#228833', '#CCBB44', '#66CCEE', '#AA3377', '#BBBBBB']\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"sans-serif\",\n",
    "    \"font.sans-serif\": [\"Helvetica\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace missing values with Zero and Group by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2fRF6mEW5QbK",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('consumos_cst.csv', parse_dates=['date'])\n",
    "df = df.set_index('date')\n",
    "df = df.groupby('cst').resample('M').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create batches of training data (12 months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequence(sequence, sequence_total, n_steps_in, n_steps_out, slide):\n",
    "    X, y = list(), list()\n",
    "    for i in range(0, len(sequence), slide):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        # check if we are beyond the sequence\n",
    "        if out_end_ix > len(sequence):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence_total[end_ix:out_end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and fit MLP model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, y, n_steps_in, n_steps_out):\n",
    "    callback = EarlyStopping(monitor='loss', patience=50)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1000, activation='relu', input_shape=(n_steps_in,)))\n",
    "    model.add(Dense(1000, activation='relu', \n",
    "                kernel_regularizer=regularizers.l1_l2(l1=1e-3, l2=1e-3),\n",
    "                bias_regularizer=regularizers.l2(1e-3)))\n",
    "    model.add(Dense(n_steps_out))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(X, y, epochs=500, verbose=1, callbacks=[callback])\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict(dataset, dataset_total, test, actual):\n",
    "    n_steps_in, n_steps_out, slide = 6, 2, 1\n",
    "    X, y = split_sequence(dataset, dataset_total, n_steps_in, n_steps_out, slide)\n",
    "    \n",
    "    # flatten input and output\n",
    "    n_input = X.shape[1] * X.shape[2]\n",
    "    X = X.reshape((X.shape[0], n_input))\n",
    "    n_output = y.shape[1] * y.shape[2]\n",
    "    y = y.reshape((y.shape[0], n_output))\n",
    "\n",
    "    model = fit(X, y, n_input, n_output)      \n",
    "    \n",
    "    mae, rmse, smape = list(), list(), list()\n",
    "    for i in range(0, len(test)-n_steps_in, n_steps_out):\n",
    "        x_input = array(test[i:n_steps_in+i])\n",
    "        x_input = x_input.reshape((1, n_input, 1))\n",
    "        pred = model.predict(x_input)\n",
    "        mae_aux, rmse_aux, smape_aux = measure_error(actual, pred[0], n_steps_in, n_steps_out, i)\n",
    "        mae.append(mae_aux), rmse.append(rmse_aux), smape.append(smape_aux)\n",
    "    \n",
    "    return mae, rmse, smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sMAPE(actual, predicted):\n",
    "    if not all([isinstance(actual, np.ndarray), \n",
    "                isinstance(predicted, np.ndarray)]):\n",
    "        actual, predicted = np.array(actual),\n",
    "        np.array(predicted)\n",
    "    return round(np.mean(np.abs(predicted - actual) / ((np.abs(predicted) + np.abs(actual))/2))*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_error(actual, pred, step_in, step_out, index):\n",
    "    #print(actual[index+step_in:index+step_in+step_out],pred,index+step_in,index+step_in+step_out)\n",
    "    #print(len(actual[index+step_in:index+step_in+step_out]),len(pred))\n",
    "    mae = mean_absolute_error(actual[index+step_in:index+step_in+step_out], pred)\n",
    "    rmse = np.sqrt(mean_squared_error(actual[index+step_in:index+step_in+step_out], pred))\n",
    "    smape = sMAPE(actual[index+step_in:index+step_in+step_out], pred)\n",
    "    return mae, rmse, smape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Data Set and Split Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.pivot_table('nr', 'date', 'cst')\n",
    "df1['Total'] = df1.sum(axis=1)\n",
    "df1 = df1.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-06 23:06:56.415948: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-10-06 23:06:56.415996: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-06 23:06:56.416026: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ubuntu): /proc/driver/nvidia/version does not exist\n",
      "2022-10-06 23:06:56.416356: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "7/7 [==============================] - 1s 24ms/step - loss: 99733400.0000\n",
      "Epoch 2/500\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 13421252.0000\n",
      "Epoch 3/500\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 6849311.0000\n",
      "Epoch 4/500\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 4953510.5000\n",
      "Epoch 5/500\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 2735343.2500\n",
      "Epoch 6/500\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 2526740.5000\n",
      "Epoch 7/500\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 2035700.5000\n",
      "Epoch 8/500\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 2034739.7500\n",
      "Epoch 9/500\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 2002254.8750\n",
      "Epoch 10/500\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 1908102.1250\n",
      "Epoch 11/500\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 1926637.7500\n",
      "Epoch 12/500\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 2167820.5000\n",
      "Epoch 13/500\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 2226006.0000\n",
      "Epoch 14/500\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 2128308.0000\n",
      "Epoch 15/500\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 2074712.6250\n",
      "Epoch 16/500\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 2177543.2500\n",
      "Epoch 17/500\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 1983703.7500\n",
      "Epoch 18/500\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1825212.3750\n",
      "Epoch 19/500\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 1844572.0000\n",
      "Epoch 20/500\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1829090.0000\n",
      "Epoch 21/500\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1735315.2500\n",
      "Epoch 22/500\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 1805817.7500\n",
      "Epoch 23/500\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 2055380.6250\n",
      "Epoch 24/500\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 2078604.7500\n",
      "Epoch 25/500\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 1830229.1250\n",
      "Epoch 26/500\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 1724021.7500\n",
      "Epoch 27/500\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 1855710.6250\n",
      "Epoch 28/500\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1676561.7500\n",
      "Epoch 29/500\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1687179.1250\n",
      "Epoch 30/500\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1753672.3750\n",
      "Epoch 31/500\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1690619.7500\n",
      "Epoch 32/500\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 1685759.7500\n",
      "Epoch 33/500\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1633442.7500\n",
      "Epoch 34/500\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 1679827.2500\n",
      "Epoch 35/500\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 1773504.6250\n",
      "Epoch 36/500\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1803517.8750\n",
      "Epoch 37/500\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1706769.1250\n",
      "Epoch 38/500\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1626469.2500\n",
      "Epoch 39/500\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1690153.3750\n",
      "Epoch 40/500\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1610159.2500\n",
      "Epoch 41/500\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1642696.0000\n",
      "Epoch 42/500\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1666474.7500\n",
      "Epoch 43/500\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1683948.5000\n",
      "Epoch 44/500\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1691476.3750\n",
      "Epoch 45/500\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1625872.7500\n",
      "Epoch 46/500\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1653264.1250\n",
      "Epoch 47/500\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1702833.7500\n",
      "Epoch 48/500\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1802094.8750\n",
      "Epoch 49/500\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 1908468.5000\n",
      "Epoch 50/500\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1902681.7500\n",
      "Epoch 51/500\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1973372.6250\n",
      "Epoch 52/500\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1637137.0000\n",
      "Epoch 53/500\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 1838228.2500\n",
      "Epoch 54/500\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 1688962.6250\n",
      "Epoch 55/500\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1706832.5000\n",
      "Epoch 56/500\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1841681.2500\n",
      "Epoch 57/500\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1905344.6250\n",
      "Epoch 58/500\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 1819478.5000\n",
      "Epoch 59/500\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 1869075.6250\n",
      "Epoch 60/500\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 1604752.7500\n",
      "Epoch 61/500\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 1680772.7500\n",
      "Epoch 62/500\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1759559.7500\n",
      "Epoch 63/500\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 1824293.0000\n",
      "Epoch 64/500\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 2106722.0000\n",
      "Epoch 65/500\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1682793.5000\n",
      "Epoch 66/500\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1769835.3750\n",
      "Epoch 67/500\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1683334.3750\n",
      "Epoch 68/500\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 1699140.2500\n",
      "Epoch 69/500\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1630348.7500\n",
      "Epoch 70/500\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 1576962.6250\n",
      "Epoch 71/500\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1534552.5000\n",
      "Epoch 72/500\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1583762.8750\n",
      "Epoch 73/500\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1768233.5000\n",
      "Epoch 74/500\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1764795.8750\n",
      "Epoch 75/500\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 1772386.2500\n",
      "Epoch 76/500\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1799891.6250\n",
      "Epoch 77/500\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 1618751.5000\n",
      "Epoch 78/500\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1521604.6250\n",
      "Epoch 79/500\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1526060.7500\n",
      "Epoch 80/500\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1550599.3750\n",
      "Epoch 81/500\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 1471888.8750\n",
      "Epoch 82/500\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1549582.2500\n",
      "Epoch 83/500\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 1626787.6250\n",
      "Epoch 84/500\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 1701380.8750\n",
      "Epoch 85/500\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1531891.1250\n",
      "Epoch 86/500\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1740578.8750\n",
      "Epoch 87/500\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 2036626.3750\n",
      "Epoch 88/500\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 2265070.0000\n",
      "Epoch 89/500\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 1786645.8750\n",
      "Epoch 90/500\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1624719.2500\n",
      "Epoch 91/500\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 1507730.1250\n",
      "Epoch 92/500\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1590465.5000\n",
      "Epoch 93/500\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 1777077.8750\n",
      "Epoch 94/500\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 1637876.3750\n",
      "Epoch 95/500\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 1529408.1250\n",
      "Epoch 96/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 26ms/step - loss: 1541920.8750\n",
      "Epoch 97/500\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 1521100.5000\n",
      "Epoch 98/500\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1402971.2500\n",
      "Epoch 99/500\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 1444545.3750\n",
      "Epoch 100/500\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1567178.8750\n",
      "Epoch 101/500\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1460554.7500\n",
      "Epoch 102/500\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1515080.1250\n",
      "Epoch 103/500\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 1451387.1250\n",
      "Epoch 104/500\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1589955.3750\n",
      "Epoch 105/500\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1653358.2500\n",
      "Epoch 106/500\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 1494127.6250\n",
      "Epoch 107/500\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1590294.2500\n",
      "Epoch 108/500\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1625921.2500\n",
      "Epoch 109/500\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1805076.6250\n",
      "Epoch 110/500\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 1940765.7500\n",
      "Epoch 111/500\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 1777900.5000\n",
      "Epoch 112/500\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1680302.5000\n",
      "Epoch 113/500\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 1655810.5000\n",
      "Epoch 114/500\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1586153.0000\n",
      "Epoch 115/500\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 1858938.7500\n",
      "Epoch 116/500\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 1597028.1250\n",
      "Epoch 117/500\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 1484186.6250\n",
      "Epoch 118/500\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1525131.3750\n",
      "Epoch 119/500\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 1565071.2500\n",
      "Epoch 120/500\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 1469118.7500\n",
      "Epoch 121/500\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 1396666.3750\n",
      "Epoch 122/500\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 1419134.7500\n",
      "Epoch 123/500\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1385922.1250\n",
      "Epoch 124/500\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1410875.2500\n",
      "Epoch 125/500\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1386618.6250\n",
      "Epoch 126/500\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 1473008.8750\n",
      "Epoch 127/500\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 1353292.1250\n",
      "Epoch 128/500\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 1444279.1250\n",
      "Epoch 129/500\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 1610881.2500\n",
      "Epoch 130/500\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 1708447.5000\n",
      "Epoch 131/500\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 1753347.6250\n",
      "Epoch 132/500\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 2225209.5000\n",
      "Epoch 133/500\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 2048600.7500\n",
      "Epoch 134/500\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 1753001.3750\n",
      "Epoch 135/500\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 1859739.0000\n",
      "Epoch 136/500\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 1598610.8750\n",
      "Epoch 137/500\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 1568095.1250\n",
      "Epoch 138/500\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1406193.6250\n",
      "Epoch 139/500\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 1371613.5000\n",
      "Epoch 140/500\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1494814.5000\n",
      "Epoch 141/500\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1428110.8750\n",
      "Epoch 142/500\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1452578.6250\n",
      "Epoch 143/500\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1406853.2500\n",
      "Epoch 144/500\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 1437633.6250\n",
      "Epoch 145/500\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1741465.7500\n",
      "Epoch 146/500\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1657212.0000\n",
      "Epoch 147/500\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 1584908.0000\n",
      "Epoch 148/500\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1746163.1250\n",
      "Epoch 149/500\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 1744412.0000\n",
      "Epoch 150/500\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 2227946.7500\n",
      "Epoch 151/500\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1857394.6250\n",
      "Epoch 152/500\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 1967402.1250\n",
      "Epoch 153/500\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 1651963.2500\n",
      "Epoch 154/500\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 1673622.2500\n",
      "Epoch 155/500\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 1873094.2500\n",
      "Epoch 156/500\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1609873.0000\n",
      "Epoch 157/500\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1833701.6250\n",
      "Epoch 158/500\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 2167235.5000\n",
      "Epoch 159/500\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 1720726.5000\n",
      "Epoch 160/500\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 1501294.8750\n",
      "Epoch 161/500\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1692901.2500\n",
      "Epoch 162/500\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 1682412.8750\n",
      "Epoch 163/500\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1851993.6250\n",
      "Epoch 164/500\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 2217153.0000\n",
      "Epoch 165/500\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 2930892.0000\n",
      "Epoch 166/500\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 2759274.7500\n",
      "Epoch 167/500\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 1707877.3750\n",
      "Epoch 168/500\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 1768601.7500\n",
      "Epoch 169/500\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1530432.2500\n",
      "Epoch 170/500\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 1456219.8750\n",
      "Epoch 171/500\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 1575099.2500\n",
      "Epoch 172/500\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1466634.2500\n",
      "Epoch 173/500\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1369737.5000\n",
      "Epoch 174/500\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 1552487.2500\n",
      "Epoch 175/500\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 1536663.7500\n",
      "Epoch 176/500\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1416055.8750\n",
      "Epoch 177/500\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 1649529.0000\n"
     ]
    }
   ],
   "source": [
    "dataset = df1[['Coimbra', 'Lisboa', 'Porto']].values\n",
    "dataset_total = df1[['Total']].values\n",
    "mae, rmse, smape = train_predict(dataset[:216], dataset_total[:216], dataset[216:], df1[['Total']][216:].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 481.89x340.157 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 3, sharex=True, figsize=(17*cm,12*cm))\n",
    "sns.boxplot(ax=axes[0], y=mae, color='thistle', showfliers=False)\n",
    "axes[0].set_title('MAE')\n",
    "axes[0].set(ylim=(0, 2000))\n",
    "sns.boxplot(ax=axes[1], y=rmse, color='lightsteelblue', showfliers=False)\n",
    "axes[1].set_title('RMSE')\n",
    "axes[1].set(ylim=(0, 2000))\n",
    "sns.boxplot(ax=axes[2], y=smape, color='darkseagreen', showfliers=False)\n",
    "axes[2].set_title('sMAPE (\\%)')\n",
    "axes[2].set(ylim=(0, 10))\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/demand_mlp_monthly_cst_boxplot.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'demand_mlp_month_cst' (list)\n"
     ]
    }
   ],
   "source": [
    "demand_mlp_month_cst = [mae,rmse,smape]\n",
    "%store demand_mlp_month_cst"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RNN_Youtube.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
